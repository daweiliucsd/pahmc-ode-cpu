# -*- coding: utf-8 -*-
"""
@author: Zheng Fang

This is the core of pahmc_ode_cpu. It receives all the user-provided specs from
'main.py' as well as the data generated by 'data_preparation.py' (or directly 
from the user if working with real data), and perform the PAHMC algorithm for 
state and parameter estimations.
"""


from pathlib import Path

from numba import jitclass, types
import numpy as np
import time

from pahmc_ode_cpu.utilities import Action


np.seterr(over='ignore')

class Core:
    """
    Below is an implementation of the Precision Annealing Hamiltonian Monte 
    Carlo methods. HMC is called from 'pahmc'. Note that functions to evaluate 
    the action, A(X), and its derivatives are in a different class.
    """

    def __init__(self, dyn, Y, dt, D, obsdim, M, tune_beta, Rm=1.0):
        """
        This class is to be instantiated in 'main.py'.

        Inputs
        ------
              dyn: an object instantiated using 'def_dynamics.Dynamics'.
                Y: the training data.
               dt: discretization interval.
                D: model degrees of freedom.
           obsdim: 1d (shapeless) numpy array of integers.
                M: number of time steps actually being used to train the model.
        tune_beta: the current beta value that is undergoing stepwise tuning.
               Rm: scalar.
        """
        self.dyn = dyn
        self.Y = Y
        self.dt = dt
        self.D = D
        self.obsdim = obsdim
        self.M = M
        self.tune_beta = tune_beta
        self.Rm = Rm

    def pa(self, Rf0, alpha, betamax, 
           n_iter, epsilon, S, mass, scaling, 
           soft_dynrange, par_start, burn=0.5):
        """
        This method does Precision Annealing using Hamiltonian Monte Carlo as
        a sampler/optimizer.

        Inputs
        ------
                  Rf0: 1d (shapeless) numpy array of floats, with length D.
                alpha: Rf = Rf0 * (alpha ** np.arange(betamax)).
              betamax: maximum beta value.
               n_iter: 1d (shapeless) numpy array of integers, with length 
                       betamax.
              epsilon: 1d (shapeless) numpy array of floats, with length 
                       betamax.
                    S: 1d (shapeless) numpy array of integers, with length 
                       betamax.
                 mass: betamax-by-3 numpy array of floats.
              scaling: 1d (shapeless) numpy array of floats, with length 
                       betamax.
        soft_dynrange: D-by-2 numpy array of floats.
            par_start: 1d (shapeless) numpy array.
                 burn: proportion of HMC samples thrown away in each beta.

        Returns:
        --------
             acceptance: numpy array of length betamax.
                 action: 2d numpy array.
        action_meanpath: numpy array of length betamax.
                   burn: proportion of HMC samples thrown away in each beta.
            FE_meanpath: numpy array of length betamax.
            ME_meanpath: numpy array of length betamax.
            par_history: 3d numpy array.
               par_mean: 2d numpy array.
                     Rf: 2d numpy array of shape (betamax, D).
                     Rm: scalar.
                 X_init: betamax-by-D-by-M numpy array.
                 X_mean: betamax-by-D-by-M numpy array.
         Xfinal_history: 3D numpy array.
        """
        # instantiate the object that evaluates action and its derivatives
        A = Action(self.dyn, self.Y, self.dt, 
                   self.D, self.obsdim, self.M, self.Rm)

        # get the Rf ladder and the unobserved dimensions
        Rf = Rf0 * (alpha ** np.arange(betamax))[:, np.newaxis]
        unobsdim = np.int64(np.setdiff1d(np.arange(self.D), self.obsdim))

        # prepare some frequently accessed variables for class 'MC'
        mass_X = np.zeros((betamax,self.D,self.M))
        mass_par = np.zeros((betamax,len(par_start)))
        for beta in range(betamax):
            mass_X[beta, self.obsdim, :] = mass[beta, 0]
            mass_X[beta, unobsdim, :] = mass[beta, 1]
            mass_par[beta, :] = mass[beta, 2]

        # initialize the outputs
        eta_avg = np.zeros(betamax)
        acceptance = np.zeros(betamax)
        action = np.zeros((betamax,np.max(n_iter)+2))
        action_meanpath = np.zeros(betamax)
        ME_meanpath = np.zeros(betamax)
        FE_meanpath = np.zeros(betamax)
        X_init = np.zeros((betamax,self.D,self.M))
        X_gd = np.zeros((betamax,self.D,self.M))
        X_mean = np.zeros((betamax,self.D,self.M))
        par_history = np.zeros((betamax,np.max(n_iter)+2,len(par_start)))
        par_mean = np.zeros((betamax,len(par_start)))
        Xfinal_history = np.zeros((betamax,np.max(n_iter)+2,self.D))

        # stepwise tuning=======================================================
        if self.tune_beta == 0:
            # dynamical initialization if tuning the first beta
            X_init[0, :, 0] = np.random.uniform(soft_dynrange[:, 0], 
                                                soft_dynrange[:, 1], (self.D,))
            X_init[0, self.obsdim, 0] = self.Y[:, 0]
            for m in range(self.M-1):  # second-order Runge-Kutta
                F = self.dt / 2 * self.dyn.field(X_init[0][:, [m]], par_start, 
                                                 self.dyn.stimuli[:, [m]])
                X_init[0][:, [m+1]] \
                  = X_init[0][:, [m]] \
                    + self.dt * self.dyn.field(X_init[0][:, [m]]+F, par_start, 
                                               self.dyn.stimuli[:, [m]])
                X_init[0, self.obsdim, m+1] = self.Y[:, m+1]

            # receive the very first parameters from the user
            par_history[0, 0, :] = par_start
        else:
            # extract results from previous beta
            file = np.load(Path.cwd()/'user_results'\
                           /f'tune_{self.dyn.name}_{self.tune_beta-1}.npz')
            X_init = file['X_mean']
            par_history[0, 0, :] = file['par_mean'][0, :]
            file.close()
        #=======================================================================

        # instantiate the MC class
        mc = MC(self.D, self.obsdim, unobsdim, self.M, 
                A, Rf, epsilon, S, mass, scaling, mass_X, mass_par)

        # perform precision annealing Hamiltonian Monte Carlo
        for beta in range(betamax):
            # kickstart
            fX = A.get_fX(X_init[beta, :, :], par_history[beta, 0, :])
            action[beta, 0] = A.action(X_init[beta, :, :], fX, Rf[beta])
            Xfinal_history[beta, 0, :] = X_init[beta, :, -1]

            # exploration first
            t0 = time.perf_counter()
            print(f'Exploring A(X) manifold for beta = {beta}... ', end='')
            X_gd[beta, :, :], par_history[beta, 1, :], action[beta, 1], eta \
              = mc.gd(X_init[beta, :, :], par_history[beta, 0, :], Rf[beta])
            print(f'finished in {time.perf_counter()-t0:.2f} seconds;')

            eta_avg[beta] = np.mean(eta)
            Xfinal_history[beta, 1, :] = X_gd[beta, :, -1]

            # then Hamiltonian Monte Carlo (exploitation)
            X0 = X_gd[beta, :, :]
            par0 = par_history[beta, 1, :]

            errcount = 0
            printflag = 0
            t0 = time.perf_counter()
            print(f'performing calculations for beta = {beta}... ', end='')
            for n in range(2, n_iter[beta]+2):
                # HMC kernel
                X, par, action[beta, n], accept, errflag \
                  = mc.hmc(X0, par0, action[beta, n-1], beta)
                X0 = X
                par0 = par

                # sanity check
                if errflag == 1:
                    errcount = errcount + 1
                    if errcount == 5 and printflag == 0:
                        print('\nWARNING: got bad values when performing ' \
                              +'leapfrog simulations... ', end='')
                        printflag = 1
                else:
                    errcount = 0

                # keep the results
                acceptance[beta] = acceptance[beta] + accept
                if n - 1 > burn * n_iter[beta]:
                    X_mean[beta, :, :] = X_mean[beta, :, :] + X
                    par_mean[beta, :] = par_mean[beta, :] + par
                Xfinal_history[beta, n, :] = X[:, -1]
                par_history[beta, n, :] = par
            print(f'finished in {time.perf_counter()-t0:.2f} seconds;')

            # finalize the acceptance rate and the mean path for current beta
            acceptance[beta] = acceptance[beta] / n_iter[beta]
            X_mean[beta, :, :] \
              = X_mean[beta, :, :] / np.ceil((1-burn)*n_iter[beta])
            par_mean[beta, :] \
              = par_mean[beta, :] / np.ceil((1-burn)*n_iter[beta])

            # calculate the action, measurement and model errors
            fX = A.get_fX(X_mean[beta, :, :], par_mean[beta, :])
            action_meanpath[beta] \
              = A.action(X_mean[beta, :, :], fX, Rf[beta])
            ME_meanpath[beta] \
              = self.Rm / (2 * self.M) \
                * np.sum((X_mean[beta, self.obsdim, :]-self.Y)**2)
            FE_meanpath[beta] \
              = np.sum(Rf[beta]/(2*self.M)\
                       *np.sum((X_mean[beta, :, 1:]-fX)**2, axis=1))

            # set starting points for the next beta
            if beta != betamax - 1:
                X_init[beta+1, :, :] = X_mean[beta, :, :]
                par_history[beta+1, 0, :] = par_mean[beta, :]

            # print the current action_meanpath and FE_meanpath
            print(f'     action (mean path) = {action_meanpath[beta]};')
            print(f'model error (mean path) = {FE_meanpath[beta]}.\n')

        return burn, self.Rm, Rf, eta_avg, acceptance, \
               action, action_meanpath, ME_meanpath, FE_meanpath, \
               X_init, X_gd, X_mean, par_history, par_mean, Xfinal_history


A_spec = types.deferred_type()
A_spec.define(Action.class_type.instance_type)

spec = [('D', types.int64), 
        ('obsdim', types.int64[:]), 
        ('unobsdim', types.int64[:]), 
        ('M', types.int64), 
        ('A', A_spec), 
        ('Rf', types.float64[:, :]), 
        ('epsilon', types.float64[:]), 
        ('S', types.int64[:]), 
        ('mass', types.float64[:, :]),
        ('scaling', types.float64[:]), 
        ('mass_X', types.float64[:, :, :]), 
        ('mass_par', types.float64[:, :])]
@jitclass(spec)
class MC:
    """
    This class implements the core steps for proposing one HMC sample. It 
    stands out from 'Core' in order to take advantage of just-in-time 
    compilation.
    """

    def __init__(self, D, obsdim, unobsdim, M, 
                 A, Rf, epsilon, S, mass, scaling, mass_X, mass_par):
        """
        This class is to be instantiated within 'Core.pa' above.

        Inputs
        ------
        Specified above.
        """
        self.D = D
        self.obsdim = obsdim
        self.unobsdim = unobsdim
        self.M = M
        self.A = A
        self.Rf = Rf
        self.epsilon = epsilon
        self.S = S
        self.mass = mass
        self.scaling = scaling
        self.mass_X = mass_X
        self.mass_par = mass_par

    def gd(self, X0, par0, Rf, eta0=0.1, tmax=1000):
        """
        This method implements batch gradient descent with adaptive learning 
        rate. It turns out that this implementation outperforms some advanced 
        algorithms such as Nesterov accelerated gradient.

        Inputs
        ------
          X0: initial path.
        par0: initial paramter set.
          Rf: current Rf.
        eta0: initial learning rate.
        tmax: maximum number of gradient descent epochs.

        Returns
        -------
             X: the final path.
           par: the final paramter set.
        action: the final action value.
           eta: the full history of learning rates.
        """
        # set initial learning rate
        eta = np.zeros(tmax+1)
        eta[0] = eta0

        # set the starting X, par
        X = X0
        par = par0

        # calculate initial action
        fX = self.A.get_fX(X, par)
        action = self.A.action(X, fX, Rf)

        accel_flag = 0
        for t in range(1, tmax+1):
            # get the gradients
            gradX = self.A.dAdX(X, par, fX, Rf, 1.0)
            gradpar = self.A.dAdpar(X, par, fX, Rf, 1.0)

            # set the starting value for current learning rate
            if accel_flag == 1:
                eta[t] = eta[t-1] * 2
            else:
                eta[t] = eta[t-1]
                accel_flag = 1

            # get the trial X, par
            X_try = X - eta[t] * gradX
            par_try = par - eta[t] * gradpar

            # get the trial action
            fX_try = self.A.get_fX(X_try, par_try)
            action_try = self.A.action(X_try, fX_try, Rf)

            counter = 0
            while action_try >= action:
                # halve the learning rate
                accel_flag = 0
                eta[t] = eta[t] / 2

                # get the trial X, par
                X_try = X - eta[t] * gradX
                par_try = par - eta[t] * gradpar

                # get the trial action
                fX_try = self.A.get_fX(X_try, par_try)
                action_try = self.A.action(X_try, fX_try, Rf)

                # return if getting stuck
                counter = counter + 1
                if counter == 100:
                    return X, par, action, eta

            # update results for the current step
            X = X_try
            par = par_try
            fX = fX_try
            action = action_try

        return X, par, action, eta

    def hmc(self, X0, par0, action0, beta):
        """
        This method generates a single HMC proposal.

        Inputs
        ------
             X0: the current path on the Markov chain.
           par0: the current parameter set on the Markov chain.
        action0: the current action evaluated from X0 and par0.
           beta: current beta value.

        Returns
        -------
             X: the output HMC proposal for the path (either a new one or X0 in 
                the case of a rejection).
           par: the output HMC proposal for the parameters (either a new one or
                par0 in the case of a rejection).
        action: the output action value evaluated from X and par.
        accept: a flag indicating whether X and par are new or old.
        """
        X = X0
        par = par0

        # generate the initial momenta
        pX0 = np.zeros((self.D,self.M))
        for a in range(len(self.obsdim)):
            for m in range(self.M):
                pX0[self.obsdim[a], m] \
                  = np.random.normal(0, np.sqrt(self.mass[beta, 0]))
        for a in range(len(self.unobsdim)):
            for m in range(self.M):
                pX0[self.unobsdim[a], m] \
                  = np.random.normal(0, np.sqrt(self.mass[beta, 1]))
        ppar0 = np.zeros(len(par))
        for b in range(len(par)):
            ppar0[b] = np.random.normal(0, np.sqrt(self.mass[beta, 2]))

        # half step for the momenta
        fX = self.A.get_fX(X, par)
        pX = pX0 - self.epsilon[beta] / 2 \
                   * self.A.dAdX(X, par, fX, self.Rf[beta], self.scaling[beta])
        ppar = ppar0 \
               - self.epsilon[beta] / 2 \
                 * self.A.dAdpar(X, par, fX, self.Rf[beta], self.scaling[beta])

        # simulate Hamiltonian dynamics
        for i in range(self.S[beta]):
            # full step for the path and parameters
            X = X + self.epsilon[beta] * pX / self.mass_X[beta, :, :]
            par = par + self.epsilon[beta] * ppar / self.mass_par[beta, :]

            # full step for the momenta except at the end of trajectory
            fX = self.A.get_fX(X, par)
            if i != self.S[beta] - 1:
                pX = pX - self.epsilon[beta] \
                          * self.A.dAdX(X, par, fX, self.Rf[beta], 
                                        self.scaling[beta])
                ppar = ppar - self.epsilon[beta] \
                              * self.A.dAdpar(X, par, fX, self.Rf[beta], 
                                              self.scaling[beta])

        # half step for the momenta to conclude the simulation
        pX = pX - self.epsilon[beta] / 2 \
                  * self.A.dAdX(X, par, fX, self.Rf[beta], self.scaling[beta])
        ppar = ppar \
               - self.epsilon[beta] / 2 \
                 * self.A.dAdpar(X, par, fX, self.Rf[beta], self.scaling[beta])

        # calculate candidate action from X and par
        action_cand = self.A.action(X, fX, self.Rf[beta])

        # calculate the change in the (original) Hamiltonian
        dH = 0
        for a in range(self.D):
            for m in range(self.M):
                dH = dH + (pX0[a, m] ** 2 - pX[a, m] ** 2) \
                          / (2 * self.mass_X[beta, a, m])
        for b in range(len(par0)):
            dH = dH + (ppar0[b] ** 2 - ppar[b] ** 2) \
                      / (2 * self.mass_par[beta, b])
        dH = dH / self.scaling[beta] + (action0 - action_cand)

        # check for numerical issues
        if (np.isnan(action_cand) == False and np.isinf(action_cand) == False \
            and np.isnan(dH) == False):
            errflag = 0
        else:
            errflag = 1

        # Metropolis acceptance/rejection rule
        if np.random.rand() < np.exp(dH):
            action = action_cand
            accept = 1
        else:
            X = X0
            par = par0
            action = action0
            accept = 0

        return X, par, action, accept, errflag

